{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUKhFL/BSFg0l61LOX7y2n"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# loaddata\n",
        "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\", header = None)\n",
        "\n",
        "# check data\n",
        "df.head()\n",
        "df.tail()\n",
        "df.info()\n",
        "df.describe()\n",
        "df.columns\n",
        "df.shape\n",
        "\n",
        "X = df.drop(60, axis=1)\n",
        "y = df[60]\n",
        "\n",
        "\n",
        "# hand miss value mean均值，median中位数，most_frequent众数\n",
        "df.isnull().sum()\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "X = imputer.fit_transform(X)\n",
        "#X= pd.DataFrame(X, columns=X.columns)\n",
        "\n",
        "\n",
        "# LabelEncoder or onehot encoder(labelencoder-分类有序, onehot-多个，无序)\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "encoder = preprocessing.OneHotEncoder()\n",
        "y= encoder.fit_transform(y)\n",
        "\n",
        "# split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
        "print(\"Train_X:\", X_train.shape)\n",
        "print(\"Train_y:\", y_train.shape)\n",
        "print(\"Test_X:\", X_test.shape)\n",
        "print(\"Test_y:\", y_test.shape)\n",
        "\n",
        "# minmax and standardscaler\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# PCA and SVD and vec\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=4)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vec = CountVectorizer()\n",
        "svd = TruncatedSVD(n_components=50)\n",
        "X_train = vec.fit_transform(X_train)\n",
        "X_train = svd.fit_transform(X_train)\n",
        "\n",
        "\n",
        "\n",
        "# pipeline and gridsearch and knn or decision tree\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"vectorizer\", CountVectorizer()),\n",
        "    (\"svd\", TruncatedSVD()),\n",
        "    (\"rf\", RandomForestClassifier()),\n",
        "    (\"dt\", DecisionTreeClassifier()),\n",
        "    (\"knn\", KNeighborsClassifier())])\n",
        "\n",
        "# Create a dictionary of hyperparameters for the pipeline with the KNN classifier\n",
        "params = {\"vectorizer__stop_words\": [None, \"english\"],\n",
        "          \"vectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
        "          \"svd__n_components\": [100, 200],\n",
        "          \"rf__n_estimators\": [10, 50, 100],\n",
        "          \"rf__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
        "          \"dt__criterion\": [\"gini\", \"entropy\"],\n",
        "          \"knn__n_neighbors\": [3,5,7,9,10, 15, 20, 25, 30]}\n",
        "\n",
        "gs = GridSearchCV(pipeline, params, scoring=\"accuracy\", cv=5) # scoring=\"f1\"\n",
        "gs.fit(X_train, y_train)\n",
        "print(gs.best_params_)\n",
        "print(gs.best_score_)\n",
        "\n",
        "pipeline.set_params(**gs.best_params_)\n",
        "pipeline.fit(X_train, y_train)\n",
        "accuracy_score(y_test, pipeline.predict(X_test))\n",
        "\n",
        "# 应用到测试集\n",
        "best_model = gs.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "ugD02iymABaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data pre-processing\n",
        "'''\n",
        "  - Dealing with Missing values\n",
        "  - Scaling Data\n",
        "  - Handling Categorical Data\n",
        "  - Feature Selection\n",
        "  - Outlier detection\n",
        "  - Dimensionality Reduction\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn import neighbors\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\", header = None)\n",
        "df.columns = [\"Class\", \"Alcohol\", \"Malic acid\", \"Ash\" , \"Alcalinity of ash\", \"Magnesium\", \"Total phenols\", \"Flavanoids\", \"Nonflavanoid phenols\", \"Proanthocyanins\", \"Color intensity\", \"Hue\", \"OD280/OD315 of diluted wines\", \"Proline\"]\n",
        "\n",
        "features = df.loc[:, \"Alcohol\": \"Proline\"]\n",
        "labels = df[\"Class\"]\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(features, labels, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# grid search and knn\n",
        "knn = neighbors.KNeighborsClassifier()\n",
        "parameters = {'n_neighbors':[1, 3, 5, 7, 11]}\n",
        "\n",
        "clf = model_selection.GridSearchCV(knn, parameters)\n",
        "clf.fit(train_features, train_labels)\n",
        "\n",
        "print(\"The best classifier is:\", clf.best_estimator_)\n",
        "print(\"The accuracy is: \", clf.best_score_)\n",
        "print(\"Its parameters are:\", clf.best_params_)\n",
        "\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "dtc = tree.DecisionTreeClassifier()\n",
        "parameters = {'criterion':[\"entropy\", \"gini\"],'max_depth':[2, 3, 4], 'min_impurity_decrease':[0.01, 0.1, 0.2]}\n",
        "\n",
        "clf = model_selection.GridSearchCV(dtc, parameters, cv=10)\n",
        "clf.fit(train_features, train_labels)\n",
        "\n",
        "print(\"The best classifier is:\", clf.best_estimator_)\n",
        "print(\"The accuracy is: \", clf.best_score_)\n",
        "print(\"Its parameters are:\", clf.best_params_)\n",
        "\n",
        "clf.best_estimator_.score(test_features, test_labels)\n",
        "\n",
        "\n",
        "\n",
        "# PCA\n",
        "from sklearn import neighbors\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(data.data, data.target, test_size=0.2, random_state=10)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "print(\"Training set size \", train_features.shape)\n",
        "\n",
        "pca = PCA(n_components=4)\n",
        "train_features = pca.fit_transform(train_features)\n",
        "test_features = pca.transform(test_features)\n",
        "print(\"Training set size after PCA\", train_features.shape)\n",
        "\n",
        "\n",
        "knn = neighbors.KNeighborsClassifier()\n",
        "parameters = {'n_neighbors': [1, 3, 5, 7, 11]}\n",
        "clf = model_selection.GridSearchCV(knn, parameters)\n",
        "clf.fit( train_features, train_labels)\n",
        "print(\"The best classifier is:\", clf.best_estimator_)\n",
        "print(\"Its accuracy is:\",clf.best_score_)\n",
        "print(\"Its parameters are:\",clf.best_params_)\n",
        "\n",
        "# using Pipelines\n",
        "from sklearn import neighbors\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "data = load_breast_cancer()\n",
        "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(data.data, data.target, test_size=0.2, random_state=10)\n",
        "\n",
        "pipe_lr = Pipeline([('scl', MinMaxScaler()), ('dr', PCA(8)), ('clf',neighbors.KNeighborsClassifier(5))])\n",
        "\n",
        "pipe_lr.fit(train_features, train_labels)\n",
        "\n",
        "predictedResults = pipe_lr.predict(test_features)\n",
        "print(metrics.accuracy_score(predictedResults, test_labels))\n",
        "# Alternatively we could substitute this line (instead of the last two lines)\n",
        "print('Test Accuracy:', pipe_lr.score(test_features, test_labels))\n",
        "\n",
        "\n",
        "\n",
        "#using pipelines for cross fold validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "results = cross_val_score(pipe_lr, train_features, train_labels, cv=10)\n",
        "print(results.mean())\n"
      ],
      "metadata": {
        "id": "ElVNmizXVT3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LabNotes"
      ],
      "metadata": {
        "id": "lmAPqCcI4LJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab07"
      ],
      "metadata": {
        "id": "hB9jWdHH4VCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider the Sonar dataset: http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)\n",
        "\n",
        "# This dataset is used for binary classification, the last column is the target.\n",
        "\n",
        "# Task:\n",
        "# 1.Load the dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\", header = None)\n",
        "\n",
        "df[60] # this is the label\n",
        "\n",
        "# 2.Use a label encoder to encode the target.\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "df[60]= encoder.fit_transform(df[60])\n",
        "\n",
        "# 3.Train-Test split 80-20. Use 42 as random seed.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = df.drop(60, axis=1)\n",
        "y = df[60]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
        "\n",
        "# 4.Train a Decision Tree on the dataset and report the accuracy result.\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tree_classifier = DecisionTreeClassifier()\n",
        "tree_classifier.fit(X_train, y_train)\n",
        "\n",
        "prediction = tree_classifier.predict(X_test)\n",
        "print(accuracy_score(y_test, prediction))\n",
        "# 0.6428571428571429\n",
        "\n",
        "# 5.Improve the Decision Tree definining a parameter grid and using Grid Search.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [1,2,3,4],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "}\n",
        "\n",
        "searcher = GridSearchCV(tree_classifier,param_grid)\n",
        "\n",
        "searcher.fit(X_train, y_train)\n",
        "\n",
        "best_params = searcher.best_params_\n",
        "\n",
        "print(\"Best parameters found: \", best_params)\n",
        "best_classifier = searcher.best_estimator_\n",
        "\n",
        "prediction = best_classifier.predict(X_test)\n",
        "print(accuracy_score(y_test, prediction))\n",
        "# Best parameters found:  {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 2} 0.6666666666666666\n",
        "\n",
        "# 6.Test a different classification model, the SVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "SVC_classifier = SVC(random_state=42)\n",
        "SVC_classifier.fit(X_train, y_train)\n",
        "\n",
        "prediction = SVC_classifier.predict(X_test)\n",
        "print(accuracy_score(y_test, prediction))\n",
        "# 0.8333333333333334\n",
        "# 7.Which has a higher accuracy?"
      ],
      "metadata": {
        "id": "yKIG_OWK4OD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## lab 08"
      ],
      "metadata": {
        "id": "-v55_MNC5_lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. dataset\n",
        "df = pd.read_csv(\"https://github.com/andvise/DataAnalyticsDatasets/blob/8e8f6475f49d2a587e4f5c76cdf0b011b22c6ac1/dataset_5000_reviews.csv?raw=true\")\n",
        "\n",
        "df.head()\n",
        "df.tail()\n",
        "\n",
        "df['Sentiment'].value_counts()\n",
        "\n",
        "# 2.preprocessing\n",
        "# labencoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X = df['Review']\n",
        "y = df['Sentiment']\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# split dataset into training set and test set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(\"Train_X:\", X_train.shape)\n",
        "print(\"Train_y:\", y_train.shape)\n",
        "print(\"Test_X:\", X_test.shape)\n",
        "print(\"Test_y:\", y_test.shape)\n",
        "\n",
        "# 3. Classification Task\n",
        "\n",
        "'''\n",
        "Create a machine learning approach using Count Vectorizer and KNN Classifier.\n",
        "Use the given parameters grid and GridSearchCV to find the optimal set\n",
        "Fit the the best model on the full training set.\n",
        "Evaluate its performance on the test set.\n",
        "Assess if adding TruncatedSVD (feature extraction) is improving the performance\n",
        "'''\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer()\n",
        "\n",
        "svd = TruncatedSVD(n_components=50)\n",
        "\n",
        "X_train = vec.fit_transform(X_train) # applying Bow\n",
        "X_train = svd.fit_transform(X_train) # applying SVD - > similar to PCA and better for sparse data\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "parameters = {'n_neighbors': [1, 3, 5],\n",
        "              'p': [1,2]}\n",
        "clf = model_selection.GridSearchCV(knn, parameters)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"The best classifier is:\", clf.best_estimator_)\n",
        "print(\"Its accuracy is:\",clf.best_score_)\n",
        "print(\"Its parameters are:\",clf.best_params_)\n",
        "\n",
        "X_test = vec.transform(X_test)\n",
        "X_test = svd.transform(X_test)\n",
        "clf.best_estimator_.score(X_test, y_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "classifier = KNeighborsClassifier(n_neighbors=5, p=1)\n",
        "classifier.fit(X_train, y_train)\n",
        "y1 = classifier.predict(X_test)\n",
        "\n",
        "print(accuracy_score(y_test, y1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulEV0v3I6Bli",
        "outputId": "cd54a0ce-af06-44e6-83dc-574a3e3a5517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_X: (4000,)\n",
            "Train_y: (4000,)\n",
            "Test_X: (1000,)\n",
            "Test_y: (1000,)\n",
            "The best classifier is: KNeighborsClassifier(p=1)\n",
            "Its accuracy is: 0.609\n",
            "Its parameters are: {'n_neighbors': 5, 'p': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.608"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lecture notes"
      ],
      "metadata": {
        "id": "3HtaJzX4hYsW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43snaLVbZJnY"
      },
      "outputs": [],
      "source": [
        "# L2pandas\n",
        "#df.attribute(dtypes、columns、shape、values、...）\n",
        "\n",
        "# L3 preprocessing in pandas\n",
        "# 1.data frame methods\n",
        "#   df.head(3), df.tail(2), df.describe(), max(), min(), std(), dropna()\n",
        "# 2. missing values\n",
        "#   dropna(), dropna(how=\"all\"),dropna(axis=1, how=\"all\"), fillna(0), isnull(), notnull()\n",
        "# 3. data selection\n",
        "#   iloc: df.iloc[0], df.iloc[-1], df.iloc[:,0],df.iloc[:, 0:2], df.iloc[1:3, 0:2], df.iloc[[0,5],[1,3]]\n",
        "# 4. groupby\n",
        "#   total sum: df.groupby('rank)['salary].sum()\n",
        "#   max and min: df.groupby('rank)['salary].max()/min()\n",
        "#   count number of elements: df.groupby('rank)['salary].count()\n",
        "#   average: df.groupby('rank)['salary].mean()\n",
        "#   median: df.groupby('rank)['salary].median()\n",
        "# 5. aggregation function\n",
        "#   min, max, count, sum, prod, mean, median\n",
        "\n",
        "# L5 - KNN\n",
        "# minmax and standard normalization\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "customer = np.array([[35, 35, 3],[22, 50, 2],[63, 200, 1], [59, 170, 1], [25, 40, 4]])\n",
        "# scalingObj = preprocessing.MinMaxScaler()\n",
        "scalingObj = preprocessing.StandardScaler()\n",
        "new_customer = scalingObj.fit_transform(customer)\n",
        "david = np.array([[37, 50, 2]])\n",
        "new_david = scalingObj.transform(david)\n",
        "\n",
        "\n",
        "# L10-11 grid, cross-validation, knn, decision tree, confusion matrix\n",
        "import pandas as pd\n",
        "from sklearn import neighbors\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\", header = None)\n",
        "df.columns = [\"Class\", \"Alcohol\", \"Malic acid\", \"Ash\" , \"Alcalinity of ash\", \"Magnesium\", \"Total phenols\", \"Flavanoids\", \"Nonflavanoid phenols\", \"Proanthocyanins\", \"Color intensity\", \"Hue\", \"OD280/OD315 of diluted wines\", \"Proline\"]\n",
        "\n",
        "features = df.loc[:, \"Alcohol\": \"Proline\"]\n",
        "labels = df[\"Class\"]\n",
        "\n",
        "# train-test split\n",
        "train_features, test_features, train_labels, test_labels = model_selection.train_test_split(features, labels, test_size = 0.2, random_state = 0)\n",
        "\n",
        "# grid search\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "\n",
        "# train the knn\n",
        "knn.fit(train_features, train_labels)\n",
        "\n",
        "predictions = knn.predict(test_features)\n",
        "\n",
        "#confusion matrix print\n",
        "cm = metrics.confusion_matrix(test_labels, predictions, labels = knn.classes_)\n",
        "\n",
        "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=knn.classes_)\n",
        "\n",
        "#disp.plot()\n",
        "#plt.show()\n",
        "\n",
        "# cross validataion of the metrics\n",
        "scores = model_selection.cross_val_score(knn, train_features, train_labels, cv=10)\n",
        "print(scores)\n",
        "\n",
        "# print mean and sd\n",
        "print(\"Accuracy: %.2f,  Standard deviation of: %.2f\" % (scores.mean(), scores.std()))\n",
        "\n",
        "# Decision tree parameters\n",
        "#criterion\n",
        "#splitter\n",
        "#max_depth\n",
        "#min_samples_split\n",
        "#min_samples_leaf\n",
        "#min_weight_fraction_leaf\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "dtc = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=2, min_samples_leaf=5)\n",
        "dtc = dtc.fit(train_features, train_labels)\n",
        "dtc.score(test_features, test_labels)\n",
        "\n",
        "# print tree\n",
        "plt.figure(figsize=(10, 10)) #Resize figure\n",
        "tree.plot_tree(dtc, feature_names= features.columns, filled = True, rounded = True)\n",
        "\n",
        "#confusion matrix\n",
        "predictions = dtc.predict(test_features)\n",
        "cm = metrics.confusion_matrix(test_labels, predictions, labels = dtc.classes_)\n",
        "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=dtc.classes_)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# grid search Decision tree\n",
        "from sklearn import tree\n",
        "\n",
        "dtc = tree.DecisionTreeClassifier()\n",
        "parameters = {'criterion':[\"entropy\", \"gini\"],'max_depth':[2, 3, 4], 'min_impurity_decrease':[0.01, 0.1, 0.2]}\n",
        "\n",
        "clf = model_selection.GridSearchCV(dtc, parameters, cv=10)\n",
        "clf.fit(train_features, train_labels)\n",
        "\n",
        "print(\"The best classifier is:\", clf.best_estimator_)\n",
        "print(\"The accuracy is: \", clf.best_score_)\n",
        "print(\"Its parameters are:\", clf.best_params_)\n",
        "\n",
        "clf.best_estimator_.score(test_features, test_labels)\n",
        "\n",
        "\n",
        "# l10\n",
        "\n",
        "import pandas as pd\n",
        "seriesA = pd.Series(['A', 'C','B'])\n",
        "seriesB = pd.Series([21, 18, 19])\n",
        "seriesC = pd.Series([4, 1, 1])\n",
        "seriesD = pd.Series(['Computing', 'Biology','Chemistry'])\n",
        "\n",
        "df = pd.DataFrame({'Grade': seriesA, 'Age': seriesB, 'DegreeYear':seriesC, 'Department': seriesD})\n",
        "\n",
        "\n",
        "grade_mapping = {'F':0, 'D':1, 'C':2, 'B':3, 'A':4}\n",
        "df['Grade'] = df['Grade'].map(grade_mapping)\n",
        "\n",
        "\n",
        "# label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "class_le = LabelEncoder()\n",
        "df['Department'] = class_le.fit_transform(df['Department'].values)\n",
        "print(df)\n",
        "\n",
        "\n",
        "# stop-words, count vectorizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "tweets = [\n",
        "\"No one is born hating another person because of the color of his skin or his background or his religion.\",\n",
        "\"People must learn to hate, and if they can learn to hate, they can be taught to love.\",\n",
        "\"For love comes more naturally to the human heart than its opposite.\"\n",
        "]\n",
        "\n",
        "# create the vectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words = 'english')\n",
        "\n",
        "# run the vectorizer\n",
        "vectorizer.fit(tweets)\n",
        "X = vectorizer.transform(tweets)\n",
        "print(X)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}